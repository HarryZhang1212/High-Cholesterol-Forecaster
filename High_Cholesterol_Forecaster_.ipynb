{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "High Cholesterol Forecaster .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNoOzJMVt_jh",
        "colab_type": "text"
      },
      "source": [
        "# High cholesterol forecaster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HP929UnuMds",
        "colab_type": "text"
      },
      "source": [
        "## Outline\n",
        "\n",
        "\n",
        "* [Part 0: Objective Summary](#Part-1:-Data-Exploration)\n",
        "* [Part 1: Data Exploration](#Part-1:-Data-Exploration)\n",
        "* [Part 2: Feature Preprocessing](#Part-2:-Feature-Preprocessing)\n",
        "* [Part 3: Model Training and Results Evaluation](#Part-3:-Model-Training-and-Result-Evaluation)\n",
        "* [Part 4: Feature Importance](#Part-4:-Feature-Selection)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z7sF7gvxTke",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Objective Summary\n",
        "Developed a prediagnostic web app to identify people at risk of high cholesterol based on their dietary behaviors which can be used by people who are unable to visit a doctor's office due to the COVID-19 pandemic or the  expenses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgNUYHrhvRjs",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgPBbK7CwSsi",
        "colab_type": "text"
      },
      "source": [
        "### Part 1.1: Understand the Raw Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfZpbClJ8ljR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import neccessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sl\n",
        "import sklearn.preprocessing as preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "pd.set_option('display.max_columns',None) \n",
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('max_colwidth',100) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkksVl1cqw3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data loading \n",
        "from google.colab import files \n",
        "uploaded = files.upload()\n",
        "df_origin = pd.read_csv('m_new.csv', index_col=0)\n",
        "df=df_origin.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3BAJ5SE_BY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Num of entries: \" + str(df.shape[0])) \n",
        "print (\"Num of features: \" + str(df.shape[1])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvJmL_sbwkwt",
        "colab_type": "text"
      },
      "source": [
        "### Part 1.2: Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXGasfuq36Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1 filter out the missing value for the target value\n",
        "df.isnull().sum()\n",
        "df1 = df[(df['Hcholes_TF']<=2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwlkQn3ThuTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1 drop feautres with missing value > 20%\n",
        "to_drop_list = ['edu_less16', 'edu_over20', 'drink_range', 'poor_diet_TF', 'lack_physical_activity_TF', 'extreme_hunger_TF', 'increased_tired_TF', 'very_thirsty_TF', 'love_suger_TF']\n",
        "df2 = df1.drop(to_drop_list, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N605LiQju-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2 format checking & remove duplication\n",
        "df2[df2.duplicated()]\n",
        "df1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUe3XVojh_7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3 remove outlier\n",
        "def iqr_outlier_rm(dt_input):\n",
        "  lq,uq=np.percentile(dt_input,[25,75])\n",
        "  lower_l=lq - 1.5*(uq-lq)\n",
        "  upper_l=uq + 1.5*(uq-lq)\n",
        "  return dt_input[(dt_input >=lower_l)&(dt_input<=upper_l)]\n",
        "  \n",
        "dt2_rm=iqr_outlier_rm(df2[['age', 'family_size',  'money_on_food', 'money_on_eatingout', 'money_on_deliveredfood', '#meal_eatoutside', '#meal_fastfood', '#meal_readytoeat',\n",
        " '#meal_frozenfood', '#hours_TV', 'income', 'diet_healthy_range', 'mike_like_range', 'mentalhealth_degree']])\n",
        "sns.boxplot(dt2_rm,orient='v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72xYhvn1hCYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#4 remove redundant feature pearson correlation coefficient test on all features \n",
        "all_features = list(df2.columns)\n",
        "df_cor = df.drop(to_drop_list, axis=1)\n",
        "corr = df_cor[all_features].corr()\n",
        "\n",
        "fig,ax0=plt.subplots(nrows=1, figsize=(10,8))\n",
        "ax0.grid(False)\n",
        "ax0.set_xlabel('X')\n",
        "ax0.set_ylabel('Randomization')\n",
        "ax0.xaxis.label.set_size(20)\n",
        "ax0.yaxis.label.set_size(20) \n",
        "sns.heatmap(corr, cmap=\"YlGnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAuIXnLPigPE",
        "colab_type": "text"
      },
      "source": [
        "### Part 1.3: Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXJpg1lIdsU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Exploratary data analysis of dietary related features\n",
        "df1['Survey_year'].loc[df1['Survey_year'] == 7] = '2011-12'\n",
        "df1['Survey_year'].loc[df1['Survey_year'] == 8] = '2013-14'\n",
        "df1['Survey_year'].loc[df1['Survey_year'] == 9] = '2015-16'\n",
        "fig,ax0=plt.subplots(nrows=1, figsize=(6,6))\n",
        "sns.set_color_codes()\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.barplot(x='Survey_year', y='num_meal_fastfood', data=df1)\n",
        "#sns.barplot(x='Survey_year', y='num_meal_readytoeat', data=df1)\n",
        "#sns.barplot(x='Survey_year', y='num_meal_frozenfood', data=df1)\n",
        "#sns.barplot(x='Survey_year', y='money_on_eatingout', data=df1,  estimator=sum)\n",
        "ax0.set_xlabel('')\n",
        "ax0.set_ylabel('')\n",
        "plt.yticks(fontsize=25)\n",
        "plt.xticks(fontsize=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlbxW5NSFKHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Distribution of target value on age & weight\n",
        "df8['Hcholes_TF']\n",
        "df_age_high = df8[(df8['Hcholes_TF']==1)]\n",
        "df_age_health = df8[(df8['Hcholes_TF']==2)]\n",
        "df7['Hcholes_TF']\n",
        "df_weight_high = df7[(df7['Hcholes_TF']==1)]\n",
        "df_weight_health = df7[(df7['Hcholes_TF']==2)]\n",
        "fig,ax0=plt.subplots(nrows=1, figsize=(6,6))\n",
        "ax0.xaxis.label.set_size(20) \n",
        "ax0.yaxis.label.set_size(20)\n",
        "sns.set_style('darkgrid')\n",
        "sns.distplot(df_age_health['age'], kde_kws={\"label\":\"Healthy\"});\n",
        "sns.distplot(df_age_high['age'], kde_kws={\"label\":\"High cholesterol\"});"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O23iEcVROPYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Distribution of target value on age & weight\n",
        "fig,ax0=plt.subplots(nrows=1, figsize=(6,6))\n",
        "ax0.xaxis.label.set_size(20) \n",
        "ax0.yaxis.label.set_size(20) \n",
        "sns.set_style('darkgrid')\n",
        "sns.distplot(df_age_health['age'], kde_kws={\"label\":\"Healthy\"});\n",
        "sns.distplot(df_age_high['age'], kde_kws={\"label\":\"High cholesterol\"});\n",
        "sns.distplot(df_weight_health['weight_pounds'], kde_kws={\"label\":\"Healthy\"});\n",
        "sns.distplot(df_weight_high['weight_pounds'], kde_kws={\"label\":\"High cholesterol\"});\n",
        "ax0.set_xlabel('')\n",
        "ax0.set_ylabel('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFqU67dj3SHD",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Feature Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmf8m7Rm6Vwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imputate missing value with mode for categorical binary features \n",
        "binary_categorical_withNan_list = ['drink_TF', 'gender','poor_TF','Hbloddpresure_TF', 'diabetes_TF','prediabetes_TF','heartfailure_TF',\n",
        " 'control_weight_now_TF', 'increase_exercise_TF', 'reduce_salt_not_TF', 'vigorous_work_TF', 'moderate_work_TF', 'vigorous_recreational_activities_TF',\n",
        " 'Moderate_recreational_activities_TF', 'trouble_sleep_TF', 'smoke_TF']\n",
        "\n",
        "df4 = df3.copy()\n",
        "for col in binary_categorical_withNan_list:\n",
        "  mode = df4[col].mode().iloc[0]\n",
        "  df4[col]=df4[col].fillna(mode)\n",
        "  df4[col].loc[df[col] != 1] = 0\n",
        "  df4[col] = df4[col] == 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IclfmWS6bgy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imputate missing value with median for numerical features \n",
        "numerical_feature_list_8400 = ['money_onfood_range', 'money_on_eatingout', 'money_on_deliveredfood']\n",
        "numerical_feature_list_5555 = ['num_meal_eatoutside', 'num_meal_fastfood', 'num_meal_readytoeat', 'num_meal_frozenfood', 'height_inches', 'weight_pounds']\n",
        "df5 = df4.copy()\n",
        "for col in numerical_feature_list_8400:\n",
        "  median = round(df5[col].median())\n",
        "  df5[col] = df5[col].fillna(median)\n",
        "  df5[col].loc[df5[col] > 8400] = median  \n",
        "for col in numerical_feature_list_5555:\n",
        "  median = round(df5[col].median())\n",
        "  df5[col] = df5[col].fillna(median)\n",
        "  df5[col].loc[df5[col] > 5555] = median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRmXkmHhS4sX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#One hot encoding for the rest categorical feature\n",
        "df6 = df_income_final.copy()\n",
        "one_hot_list =['Survey_year','diet_healthy_range', 'mike_like_range','pay_attention_nutrient_TF','mentalhealth_degree','overweight_TF']\n",
        "df7 = pd.get_dummies(df6, columns= one_hot_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3st-y9Fos6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bucketing skewed feature income \n",
        "df_income['income'].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])\n",
        "bin_income = [0, 5, 8, 14, 15]\n",
        "income_range = ['0-5', '5-8', '8-14', '14-15']\n",
        "df_income['income'] = pd.cut(df_income['income'], bin_income, labels= income_range)\n",
        "df_income_plotonly = df_income['income'].str.get_dummies(sep=',').count()\n",
        "df_income_plotonly.plot.bar(color = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwHEb1tcbpy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#feature generation BMI\n",
        "df8= df7.copy()\n",
        "def BMI(series):\n",
        "  weight = series[\"weight_pounds\"]\n",
        "  height = series[\"height_inches\"]\n",
        "  BMI = weight*703/height**2\n",
        "  return BMI\n",
        "\n",
        "df8[\"BMI\"] = df8.apply(BMI,axis=1)\n",
        "df8 = df8.drop([\"weight_pounds\", \"height_inches\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGZbpqdwpw7E",
        "colab_type": "text"
      },
      "source": [
        "### Part 2.2: Data scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jylcG2KiqASK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get ground truth data \n",
        "y = np.where(df8['Hcholes_TF'] == 1 ,1,0)\n",
        "X = df8.drop(['Hcholes_TF'], axis=1)\n",
        "#Standarized the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train) \n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpuYM0uv4OnC",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbc4QKvIKjOb",
        "colab_type": "text"
      },
      "source": [
        "### Part 3.1: Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDQ4AioSwucP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Under sampling for imbalanced data \n",
        "#Data splitting\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "RandomUnderSampler = RandomUnderSampler()\n",
        "X_resampled, y_resampled = RandomUnderSampler.fit_sample(X, y)\n",
        "\n",
        "# Reserve 20% for testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_resampled, y_resampled, test_size=0.2)\n",
        "print('training data has %d observation with %d features'% X_train.shape)\n",
        "print('test data has %d observation with %d features'% X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yElNBfxZG8aR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#title build models \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC \n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "classifier_logistic = LogisticRegression()\n",
        "classifier_KNN = KNeighborsClassifier()\n",
        "classifier_SVC = SVC()\n",
        "classifier_RF = RandomForestClassifier()\n",
        "classifier_XGB = XGBClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-0J4kLHsdvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use 10-fold Cross Validation to get the accuracy for different models\n",
        "model_names = ['Logistic Regression','Random Forest','SVM', 'KNN', 'XGB']\n",
        "model_list = [classifier_logistic, classifier_KNN, classifier_RF, classifier_SVC, classifier_XGB]\n",
        "count = 0\n",
        "\n",
        "for classifier in model_list:\n",
        "    cv_score = model_selection.cross_val_score(classifier, X_train, y_train, cv=10)\n",
        "    print(cv_score)\n",
        "    print('Model accuracy of %s is: %.3f'%(model_names[count],cv_score.mean()))\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRWk-fvsKmVm",
        "colab_type": "text"
      },
      "source": [
        "###Part 3.2: Model Training and Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS34FC2DKwth",
        "colab_type": "text"
      },
      "source": [
        "###Part 3.3: Use Grid Search to Find Optimal Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvcvvxyNK4Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "# helper function for printing out grid search results \n",
        "def print_grid_search_metrics(gs):\n",
        "    print (\"Best score: %0.3f\" % gs.best_score_)\n",
        "    print (\"Best parameters set:\")\n",
        "    best_parameters = gs.best_params_\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIfNzCqaZglW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for Logistic Regression Regularization \n",
        "parameters = {\n",
        "    'penalty':('l1', 'l2'), \n",
        "    'C':(0.01, 0.1, 1)\n",
        "}\n",
        "Grid_LR = GridSearchCV(LogisticRegression(),parameters, cv=10)\n",
        "Grid_LR.fit(X_train, y_train)\n",
        "print_grid_search_metrics(Grid_LR)\n",
        "best_LR_model = Grid_LR.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6KkW6Us1FD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for Random Forest--choose the best number of trees\n",
        "parameters = {\n",
        "    'n_estimators' : [50,100,200]\n",
        "}\n",
        "Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=10)\n",
        "Grid_RF.fit(X_train, y_train)\n",
        "print_grid_search_metrics(Grid_RF)\n",
        "best_RF_model = Grid_RF.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixwgqpFSEEA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for KNN --choose the best k\n",
        "parameters = {\n",
        "    'n_neighbors':[1,10,100] \n",
        "}\n",
        "Grid_KNN = GridSearchCV(KNeighborsClassifier(),parameters, cv=10)\n",
        "Grid_KNN.fit(X_train, y_train)\n",
        "print_grid_search_metrics(Grid_KNN)\n",
        "best_KNN_model = Grid_KNN.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBc9adj2rVfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible hyperparamter options for XGBoost \n",
        "parameters = {\n",
        "    'max_depth': [5, 10, 15, 20, 25],\n",
        "    'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
        "    'n_estimators': [50, 100, 200, 300, 500],\n",
        "    'min_child_weight': [0, 2, 5, 10, 20],\n",
        "    'max_delta_step': [0, 0.2, 0.6, 1, 2],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],\n",
        "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
        "    'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n",
        "    'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],\n",
        "    'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]\n",
        "\n",
        "}\n",
        "Grid_XGB = GridSearchCV(classifier_XGB,parameters, cv=10)\n",
        "Grid_XGB.fit(X_train, y_train)\n",
        "print_grid_search_metrics(Grid_XGB)\n",
        "best_XGB_model = Grid_XGB.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTvNuvgHgopJ",
        "colab_type": "text"
      },
      "source": [
        "## 4.1: Evaluate all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6yia6-JQEsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# calculate accuracy, precision and recall, [[tn, fp],[]]\n",
        "def cal_evaluation(classifier, cm):\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] #新家的这一句是normalized的结果了\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    tp = cm[1][1]\n",
        "    accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
        "    precision = tp / (tp + fp + 0.0)\n",
        "    recall = tp / (tp + fn + 0.0)\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    print (classifier)\n",
        "    print (\"Accuracy is: %0.3f\" % accuracy)\n",
        "    print (\"precision is: %0.3f\" % precision)\n",
        "    print (\"recall is: %0.3f\" % recall)\n",
        "    print (\"tn is: %0.3f\" % tn)\n",
        "    print (\"fp is: %0.3f\" % fp)\n",
        "    print (\"fn is: %0.3f\" % fn)\n",
        "    print (\"tp is: %0.3f\" % tp)\n",
        "    print (\"f1 score is: %0.3f\" % f1)\n",
        "   \n",
        "\n",
        "# print out confusion matrices\n",
        "def draw_confusion_matrices(confusion_matricies):\n",
        "    class_names = ['Healthy','Hight cholesterol']\n",
        "    for cm in confusion_matrices:\n",
        "        classifier, cm = cm[0], cm[1]\n",
        "        cal_evaluation(classifier, cm)\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap(\"YlGnBu\"))\n",
        "        #plt.title('Confusion matrix for %s' % classifier)\n",
        "        fig.colorbar(cax)\n",
        "        ax.set_xticklabels([''] + class_names)\n",
        "        ax.set_yticklabels([''] + class_names)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual values')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTt2k7wmKlC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change threshod from 0.5 to 0.3\n",
        "%matplotlib inline\n",
        "new =  (best_RF_model.predict_proba(X_test)[:,1]>=0.3).astype(int)\n",
        "# Confusion matrix, accuracy, precison and recall for random forest and logistic regression\n",
        "confusion_matrices = [\n",
        "    (\"Random Forest\", confusion_matrix(y_test,new)),\n",
        "    (\"Logistic Regression\", confusion_matrix(y_test,new)),\n",
        "    (\"KNN\", confusion_matrix(y_test,best_KNN_model.predict(X_test))),\n",
        "     (\"XGBoost\", confusion_matrix(y_test,xgb_model.predict(X_test)))\n",
        "]\n",
        "\n",
        "draw_confusion_matrices(confusion_matrices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CmJPhbNsXDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn import metrics\n",
        "\n",
        "model_names = ['Logistic Regression','Random Forest','SVM', 'KNN', 'XGB']\n",
        "model_list = [classifier_logistic, classifier_RF, classifier_SVC, classifier_KNN, classifier_XGB]\n",
        "count = 0\n",
        "\n",
        "for classifier in model_list:\n",
        "    y_pred = classifier.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "    print('Model fpr of %s is: %.3f'%(model_names[count], fpr)\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzGIiHoTQo37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ROC curve of all models\n",
        "fig,ax0=plt.subplots(nrows=1, figsize=(6,6))\n",
        "sns.set_style('darkgrid')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='Random Forest')\n",
        "plt.plot(fpr_xgb, tpr_xgb, label='XGBoost')\n",
        "plt.plot(fpr_lr, tpr_lr, label='Logistic regression')\n",
        "plt.plot(fpr_knn, tpr_knn, label='KNN')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve', fontsize=20)\n",
        "plt.legend(loc='best')\n",
        "plt.rc('legend',fontsize=20)\n",
        "plt.xticks(rotation=0)\n",
        "ax0.set_xlabel('False positive rate', fontsize=15)\n",
        "ax0.set_ylabel('True positive rate',fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS2a4Nw1QvpL",
        "colab_type": "text"
      },
      "source": [
        "# Part 4: Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgBT6stDQ5L_",
        "colab_type": "text"
      },
      "source": [
        "### Part 4.1:  Random Forest Model - Feature Importance Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGjypeoWQ7ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "importances = best_RF_model.feature_importances_\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature importance ranking by Random Forest Model:\")\n",
        "for k,v in sorted(zip(map(lambda x: round(x, 4), importances), X.columns), reverse=True):\n",
        "    print (v + \": \" + str(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLXQlT_lbqWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature importancefor young people \n",
        "df8.columns = df8_col_importance\n",
        "df_young = df8[(df8['age']<=24)]\n",
        "y_young = np.where(df_young['Hcholes_TF'] == 1 ,1,0)\n",
        "X_young = df_young.drop(['Hcholes_TF'], axis=1)\n",
        "forest = RandomForestClassifier()\n",
        "forest.fit(X_young, y_young)\n",
        "importances_young = forest.feature_importances_\n",
        "df_importance_young = pd.DataFrame(importances_young, index=X_young.columns.values, columns=['feature importance']).sort_values('feature importance', ascending=False)\n",
        "df_young_importance = df_importance_young[(df_importance_young['feature importance']> 0.043)].reset_index() \n",
        "fig,ax0=plt.subplots(nrows=1, figsize=(6,6))\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_color_codes()\n",
        "certain = ['gray' if (x != df_young_importance.iloc[4][1]) else 'red' for x in df_young_importance['feature importance'] ]\n",
        "sns.barplot(y = 'index' , x='feature importance', data=df_young_importance, palette=certain)\n",
        "ax0.set_xlabel('')\n",
        "ax0.set_ylabel('')\n",
        "plt.yticks(fontsize=20)\n",
        "plt.xticks(fontsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}